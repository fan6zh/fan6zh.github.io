<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Dr Fan Zhang | Publications</title>
  <meta name="description" content="A beautiful Jekyll theme for academics">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Dr Fan Zhang</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
              <li class="nav-item ">
                  <a class="nav-link" href="/assets/pdf/vitae.pdf">
                    Curriculum Vitae
                    
                  </a>
              </li>
            
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    Teaching
                    
                  </a>
              </li>
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>


<p><br /></p>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://ewh.ieee.org/soc/ras/conf/fullysponsored/icra/ICRA2020/www.icra2020.org/index.html" target="_blank">
          ICRA
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="zhang2020ICRA" class="col p-0">
      <h5 class="title mb-0"> Learning Grasping Points for Garment Manipulation in Robot-Assisted Dressing</h5>
      <div class="author">
                
									<nobr><em>Fan Zhang</em>,</nobr>
              
									and
                
                  <nobr><a href="https://www.imperial.ac.uk/people/y.demiris" target="_blank">Yiannis Demiris</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          IEEE International Conference on Robotics and Automation (ICRA), 2020
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zhang2020ICRA-abstract" role="button" aria-expanded="false" aria-controls="zhang2020ICRA-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/abstract/document/9196994" target="_blank">Paper</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=yVScTBbw7E4" target="_blank">Video</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=HFSLJSjnbi8" target="_blank">Talk</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="zhang2020ICRA-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Assistive robots have the potential to provide tremendous support for disabled and elderly people in their daily dressing activities. Recent studies on robot-assisted dressing usually simplify the setup of the initial robot configuration by manually attaching the garments on the robot end-effector and positioning them close to the user's arm. A fundamental challenge in automating such a process for robots is computing suitable grasping points on garments that facilitate robotic manipulation. In this paper, we address this problem by introducing a supervised deep neural network to locate a pre-defined grasping point on the garment, using depth images for their invariance to color and texture. To reduce the amount of real data required, which is costly to collect, we leverage the power of simulation to produce large amounts of labeled data. The network is jointly trained with synthetic datasets of depth images and a limited amount of real data. We introduce a robot-assisted dressing system that combines the grasping point prediction method, with a grasping and manipulation strategy which takes grasping orientation computation and robot-garment collision avoidance into account. The experimental results demonstrate that our method is capable of yielding accurate grasping point estimations. The proposed dressing system enables the Baxter robot to autonomously grasp a hospital gown hung on a rail, bring it close to the user and successfully dress the upper-body. 
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>

			
			
			</ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2019</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.ieee-ras.org/publications/t-ro" target="_blank">
          T-RO
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="zhang2019TRO" class="col p-0">
      <h5 class="title mb-0">Probabilistic Real-Time User Posture Tracking for Personalized Robot-Assisted Dressing</h5>
      <div class="author">
                
									<nobr><em>Fan Zhang</em>,</nobr>
              
                  <nobr><a href="https://www.imperial.ac.uk/people/a.cully" target="_blank">Antoine Cully<nobr><em></em></nobr></a>,</nobr>
                
              
									and
                
                  <nobr><a href="https://www.imperial.ac.uk/people/y.demiris" target="_blank">Yiannis Demiris</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          IEEE Transactions on Robotics (T-RO)
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zhang2019TRO-abstract" role="button" aria-expanded="false" aria-controls="zhang2019TRO-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/abstract/document/8685136" target="_blank">Paper</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=9S8joEXxDCM" target="_blank">Video</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://tv.theiet.org/?eventvideoid=13345" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="zhang2019TRO-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
Robotic solutions to dressing assistance have the potential to provide tremendous support for elderly and disabled people. However, unexpected user movements may lead to dressing failures or even pose a risk to the user. Tracking such user movements with vision sensors is challenging due to severe visual occlusions created by the robot and clothes. We propose a probabilistic tracking method using Bayesian networks in latent spaces, which fuses robot end-effector positions and force information to enable camera-less and real-time estimation of the user postures during dressing. The latent spaces are created before dressing by modeling the user movements with a Gaussian Process Latent Variable Model, taking the user's movement limitations into account. We introduce a robot-assisted dressing system that combines our tracking method with hierarchical multi-task control to minimize the force between the user and the robot. The experimental results demonstrate the robustness and accuracy of our tracking method. The proposed method enables the Baxter robot to provide personalized dressing assistance in putting on a sleeveless jacket for users with (simulated) upper-body impairments.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2018</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://sites.google.com/a/robot-learning.org/corl2017/corl2018" target="_blank">
          JMES
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="zhang2018preoperative" class="col p-0">
      <h5 class="title mb-0">Preoperative Optimization of the Surgical Robot Considering Internal Diversity of Workspace</h5>
      <div class="author">
                
              
                  <nobr>Zhiyuan Yan</a>,</nobr>
                
                  <nobr>Zhijiang Du</a>,</nobr>

                  <nobr><em>Fan Zhang</em>,</nobr>
            
									and
                
                  <nobr>Weidong Wang</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zhang2018preoperative-abstract" role="button" aria-expanded="false" aria-controls="zhang2018preoperative-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://journals.sagepub.com/doi/full/10.1177/0954406217699019" target="_blank">Paper</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="zhang2018preoperative-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Surgical robots have increased in popularity, and their performance is closely related to the robotic positioning before surgery. Many recent studies in preoperative planning have focused on the pose selection of the robot and the port placement. However, it is difficult to position the surgical robot simply based on experience. To solve this problem, the surgical workspace is subdivided into several subspaces with different weights. Global isotropy index and cooperation capability index are proposed to reflect the performance of the surgical robot and used as optimization functions. Particle swarm optimization is used to optimize the setup parameters. Based on different weight distributions, setup parameters can be automatically given and sent to the simulation system to display the setup and guide the robot positioning. The results show that the setup optimization considering the internal diversity of workspace is capable of satisfying the detailed requirements of robotic surgery and effectively guide the robotic surgery setup.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>

<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://iclr.cc/Conferences/2018" target="_blank">
          ICLR
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2018ICLR" class="col p-0">
      <h5 class="title mb-0">Adapting to Continuously Shifting Domains</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://scholar.google.com/citations?user=nABXo3sAAAAJ&hl=en" target="_blank">Eric Tzeng<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://www.cc.gatech.edu/~judy/" target="_blank">Judy Hoffman</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          International Conference on Learning Representations (ICLR) Workshop, 2018
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2018ICLR-abstract" role="button" aria-expanded="false" aria-controls="bobu2018ICLR-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CUA/bobu2018ICLR.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CUA/bobu2018ICLR_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2018ICLR-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Domain adaptation typically focuses on adapting a model from a single source domain to a target domain. However, in practice, this paradigm of adapting from one source to one target is limiting, as different aspects of the real world such as illumination and weather conditions vary continuously and cannot be effectively captured by two static domains. Approaches that attempt to tackle this problem by adapting from a single source to many different target domains simultaneously are consistently unable to learn across all domain shifts. Instead, we propose an adaptation method that exploits the continuity between gradually varying domains by adapting in sequence from the source to the most similar target domain. By incrementally adapting while simultaneously efficiently regularizing against prior examples, we obtain a single strong model capable of recognition within all observed domains. Our method is applicable on a wide variety of learning settings, including visual classification and reinforcement learning in a video game domain.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


			</ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2016</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
			
		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.miccai2016.org/en/" target="_blank">
          MICCAI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="dalca2016MICCAI" class="col p-0">
      <h5 class="title mb-0">Patch-Based Discrete Registration of Clinical Brain Images</h5>
      <div class="author">

                  <nobr><a href="http://www.mit.edu/~adalca/" target="_blank">Adrian Dalca<nobr><em></em></nobr></a>,</nobr>
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://www.massgeneral.org/doctors/17477/natalia-rost" target="_blank">Natalia S. Rost</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.csail.mit.edu/polina/" target="_blank">Polina Golland</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          International Conference on Medical Image Computing and Computer Assisted Intervention 2016
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#dalca2016MICCAI-abstract" role="button" aria-expanded="false" aria-controls="dalca2016MICCAI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/PBR/dalca2016MICCAI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/adalca/patchRegistration" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/PBR/dalca2016MICCAI_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="dalca2016MICCAI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
						We introduce a method for registration of brain images acquired in clinical settings. The algorithm relies on three-dimensional patches in a discrete registration framework to estimate correspondences. Clinical images present significant challenges for computational analysis. Fast acquisition often results in images with sparse slices, severe artifacts, and variable fields of view. Yet, large clinical datasets hold a wealth of clinically relevant information. Despite significant progress in image registration, most algorithms make strong assumptions about the continuity of image data, failing when presented with clinical images that violate these assumptions. In this paper, we demonstrate a non-rigid registration method for aligning such images. The method explicitly models the sparsely available image information to achieve robust registration. We demonstrate the algorithm on clinical images of stroke patients. The proposed method outperforms state of the art registration algorithms and avoids catastrophic failures often caused by these images. We provide a freely available open source implementation of the algorithm.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


			</ol>
    </div>
  </div>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2020 Andreea Bobu.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
