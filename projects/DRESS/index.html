<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Dr Fan Zhang | Robot-Assisted Dressing</title>
  <meta name="description" content="My personal website.
">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/projects/DRESS/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Dr Fan Zhang</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/assets/pdf/vitae.pdf">
                    Curriculum Vitae
                    
                  </a>
              </li>
            
          
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/travel/">
                    Travel
                    
                  </a>
              </li>
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    <nav aria-label="breadcrumb">
  <ol class="breadcrumb p-0 text-lowercase">
    <li class="breadcrumb-item"><a href="/">home</a></li>
    <li class="breadcrumb-item"><a href="/projects">Projects</a></li>
    <li class="breadcrumb-item less">Robot-Assisted Dressing</li>
  </ol>
</nav>

<div class="row m-0" style="width: 100%;">
  <div class="col-sm-12 p-0">
    <h1>Robot-Assisted Dressing</h1>
    <h5 class="mb-4">for impaired patients</h5>
  </div>
	<div><p style="width:100%;"><img src="../../assets/img/dress.png"  height="230" width="800" class="post_thumb" ></p>
		<p align="justify">Summary: Assistive robots have the potential to provide tremendous support for disabled and elderly people in their daily dressing activities. Recent studies on robot-assisted dressing usually simplify the setup of the initial robot configuration by manually attaching the garments on the robot end-effector and positioning them close to the user's arm. A fundamental challenge in automating such a process for robots is computing suitable grasping points on garments that facilitate robotic manipulation. In this paper, we address this problem by introducing a supervised deep neural network to locate a pre-defined grasping point on the garment, using depth images for their invariance to color and texture. To reduce the amount of real data required, which is costly to collect, we leverage the power of simulation to produce large amounts of labeled data. The network is jointly trained with synthetic datasets of depth images and a limited amount of real data. We introduce a robot-assisted dressing system that combines the grasping point prediction method, with a grasping and manipulation strategy which takes grasping orientation computation and robot-garment collision avoidance into account. The experimental results demonstrate that our method is capable of yielding accurate grasping point estimations. The proposed dressing system enables the Baxter robot to autonomously grasp a hospital gown hung on a rail, bring it close to the user and successfully dress the upper-body. <p>

<p align="justify">Unexpected user movements may lead to dressing failures or even pose a risk to the user. Tracking such user movements with vision sensors is challenging due to severe visual occlusions created by the robot and clothes. We propose a probabilistic tracking method using Bayesian networks in latent spaces, which fuses robot end-effector positions and force information to enable camera-less and real-time estimation of the user postures during dressing. The latent spaces are created before dressing by modeling the user movements with a Gaussian Process Latent Variable Model, taking the user's movement limitations into account. We introduce a robot-assisted dressing system that combines our tracking method with hierarchical multi-task control to minimize the force between the user and the robot. The experimental results demonstrate the robustness and accuracy of our tracking method. The proposed method enables the Baxter robot to provide personalized dressing assistance in putting on a sleeveless jacket for users with (simulated) upper-body impairments.
 </p>
</div>

<div class="row m-0 text text-justify" style="width:100%;">
	<h5>Publications:</h5>
<ul style="margin-left:10px">
	<li><div id="zhang2020ICRA" class="col p-0">
										<nobr><em>Fan Zhang</em>,</nobr>
              
									and
                
                  <nobr><a href="https://www.imperial.ac.uk/people/y.demiris" target="_blank">Yiannis Demiris</a>.</nobr>
				<a href="https://ieeexplore.ieee.org/abstract/document/9196994" target="_blank">Learning Grasping Points for Garment Manipulation in Robot-Assisted Dressing</a>.
				<i>IEEE International Conference on Robotics and Automation (ICRA), 2020</i>.
		</div></li>
	<li><div id="zhang2019TRO" class="col p-0">
																<nobr><em>Fan Zhang</em>,</nobr>
              
                  <nobr><a href="https://www.imperial.ac.uk/people/a.cully" target="_blank">Antoine Cully<nobr><em></em></nobr></a>,</nobr>
                
              
									and
                
                  <nobr><a href="https://www.imperial.ac.uk/people/y.demiris" target="_blank">Yiannis Demiris</a>.</nobr>
				<a href="https://ieeexplore.ieee.org/abstract/document/8685136" target="_blank">Probabilistic Real-Time User Posture Tracking for Personalized Robot-Assisted Dressing</a>.
				<i>IEEE Transactions on Robotics (T-RO), 2019</i>.
		</div></li>
	<li><div id="zhang2017iros" class="col p-0">
																<nobr><em>Fan Zhang</em>,</nobr>
              
                  <nobr><a href="https://www.imperial.ac.uk/people/a.cully" target="_blank">Antoine Cully<nobr><em></em></nobr></a>,</nobr>
                
              
									and
                
                  <nobr><a href="https://www.imperial.ac.uk/people/y.demiris" target="_blank">Yiannis Demiris</a>.</nobr>
				<a href="https://ieeexplore.ieee.org/abstract/document/8206206" target="_blank">Personalized Robot-Assisted Dressing using User Modeling in Latent Spaces</a>.
				<i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017</i>.
		</div></li>
  </ul>

</div>

  <div class="row m-0" style="width:100%;">
	<h5>Videos:</h5>
	<ul style="margin-left:10px">
        <iframe width="45%" height="200" src="https://www.youtube.com/embed/HFSLJSjnbi8" frameborder="0" allowfullscreen></iframe>
	<iframe width="45%" height="200" src="https://www.youtube.com/embed/yVScTBbw7E4" frameborder="0" allowfullscreen></iframe>
	<iframe width="45%" height="200" src="https://www.youtube.com/embed/9S8joEXxDCM" frameborder="0" allowfullscreen></iframe>
	<iframe width="45%" height="200" src="https://www.youtube.com/embed/FT5VKm3kgoM" frameborder="0" allowfullscreen></iframe>
  </div> 

  <div class="row m-0 text text-justify" style="width:100%;">
	<h5>Awards, Press and Talks:</h5>
<ul style="margin-left:10px">
	<li><a href="https://queenmaryroboticsaward.blogspot.com/p/previous-recipients.html" target="_blank">The Queen Mary UK Best PhD in Robotics Award 2020 1st place</a>.</li> 
	<li>Press: Robotic Nurse That Helps You Dress Could Aid Staff Shortage on&nbsp;<a href="https://www.bloomberg.com/news/articles/2019-08-22/nurse-named-baxter-that-helps-you-dress-could-aid-staff-shortage">Bloomberg</a>,&nbsp;<a href="https://www.thetimes.co.uk/article/baxter-the-nursebot-to-help-care-for-ageing-population-9nj57xqvl">The Times</a>,&nbsp;<a href="https://www.dailymail.co.uk/health/article-7386631/Scientists-create-Baxter-robot-assist-elderly-amid-shortage-nurses.html">Daily Mail</a>,&nbsp;<a href="https://www.telegraph.co.uk/technology/2019/08/22/meet-baxter-robot-nurse-could-help-dress-elderly-aid-nhs-staff/">Telegraph</a>,&nbsp;<a href="https://www.scmp.com/lifestyle/health-wellness/article/3024028/how-robot-nurses-could-help-care-worlds-elderly-and">South China Morning Post</a>.</li> 
	<li>Talks: TechBeat talk (Aug 2021,&nbsp;<a href="https://www.techbeat.net/talk-info?id=576">video</a>); Intelligent Robot Seminar (Chinese Association for Artificial Intelligence, with more than 150,000 live audience, Jun 2020,&nbsp;<a href="https://dl.ccf.org.cn/video/videoDetail.html?_ack=1&id=5531122081302532">video</a>); IET Conference Human Motion Analysis for Healthcare Applications (July 2019,&nbsp;<a href="https://tv.theiet.org/?eventvideoid=13345">video</a>); The Hamlyn Centre, Imperial College London (Nov 2017); The 2nd UK Robot Manipulation Workshop (Jul 2017)..</li> 
	<li><a href="https://github.com/andreea7b/beta_adaptive_pHRI" target="_blank">Code</a> for confidence-aware learning from physical human corrections and demonstrations on a Jaco 7DOF robotic manipulator.</li> 
	
  </div> 

</div>

<div class="container-fluid p-0 text-justify">
  

</div>

  </div>

  <!-- Footer -->
  <!--<footer>
    &copy; Copyright 2020 Fan Zhang. -->
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- GitHub Stars -->
  <script src="/assets/js/github-stars.js"></script>
  <script type="text/javascript">
    
      
    
      
    
      
    
      
        
        githubStars("eaplatanios/symphony-mt", function(stars) { $("#curriculum-learningeaplatanios-symphony-mt-stars").text('' + stars); });
      
    
      
        
        githubStars("eaplatanios/jelly-bean-world", function(stars) { $("#jelly-bean-worldeaplatanios-jelly-bean-world-stars").text('' + stars); });
      
    
      
        
        githubStars("eaplatanios/symphony-mt", function(stars) { $("#machine-translationeaplatanios-symphony-mt-stars").text('' + stars); });
      
    
      
    
      
    
      
        
        githubStars("eaplatanios/tensorflow_scala", function(stars) { $("#TensorFlow-Scalaeaplatanios-tensorflow_scala-stars").text('' + stars); });
      
    
  </script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54519238-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
